\section{Empirical Methodology}
\label{sec:em}

We begin by assuming an \acs{MLR} because \acs{MLR} is the most widely used model in econometrics, serving as the baseline to other regression models \cite{christopher_2006, wooldridge_2020}. The \acs{PRF} for a cross-sectional sample is given by
\begin{equation}\label{eq:mlr_prf}
{Y_{i}}=\beta_{0}+\beta_{1}{X_{1i}}+\beta_{2}X_{2i}+\dots+\beta_{k}X_{ki}+\epsilon_{i}\eqcomma\quad i=1,2,\dots,n
\end{equation}

In Equation (\ref{eq:mlr_prf}), the subscript $i$ indicates the $i$-th of the $n$ observations in the sample\footnote{We will be using a lot of equations with subscript $i$ having the same meaning. Thus, we will omit $i=1,2,\dots,n$ in the equations for the rest of the paper.}. $Y_{i}$ is the regressand. $X_{1i}, X_{2i},\dots, X_{ki}$ are the $k$ regressors. The variable $\epsilon_{i}$ is the error term or stochastic disturbance of the regression, given $n$ observations.

Then, our \acs{SRF} can be obtained using \acs{OLS} estimation:
\begin{equation}\label{eq:mlr_srf}
\begin{aligned}
\hat{Y_{i}}&=\hat{\beta_{0}}+\hat{\beta_{1}}X_{1i}+\dots+\hat{\beta_{k}}X_{ki}\eqcomma
\\
\hat{\epsilon_{i}}&=Y_{i}-\hat{Y_{i}}
\end{aligned}
\end{equation} 


$\hat{Y_{i}}$ and $\hat{\epsilon_{i}}$ are called the \acs{OLS} predicted values and residuals of the error term $\epsilon_{i}$. The \acs{OLS} estimators $\hat{\beta_{0}},\hat{\beta_{1}},\dots,\hat{\beta_{k}}$ and $\hat{\epsilon_{i}}$ can be computed from a sample of $n$ observations of $(X_{1i},X_{2i},\dots,X_{ki})$ using multivariable calculus and linear algebra. The formula and the proof are shown in Appendix \ref{appendix:ols}. 

Yet, as we have seen in \hyperref[sec:data]{\text{Data}} Section, \acs{MLR} may not be the best-fitted model for our data. That is why we introduce \acs{MNR} in this paper. Here, we focus on two nonlinear functions for a single independent variable: polynomials and logarithms.
The polynomial regression model of degree $r$ is written as:
\begin{equation}\label{eq:mlr_polynomial}
{Y_{i}}=\beta_{0}+\beta_{1}{X_{i}}+\beta_{2}X_{i}^2+\dots+\beta_{r}X_{i}^r+\epsilon_{i}
\end{equation}

For logarithms, there are three linear forms after transformed from \acs{MNR}:
\begin{align}
{Y_{i}}&=\beta_{0}+\beta_{1}\ln({X_{i}})+\epsilon_{i}\label{eq:mlr_linear_log}\\
\ln({Y_{i}})&=\beta_{0}+\beta_{1}({X_{i}})+\epsilon_{i}\label{eq:mlr_log_linear}\\
\ln({Y_{i}})&=\beta_{0}+\beta_{1}\ln({X_{i}})+\epsilon_{i}\label{eq:mlr_log_log}
\end{align}

Equation (\ref{eq:mlr_linear_log}), (\ref{eq:mlr_log_linear}) and (\ref{eq:mlr_log_log}) are referred to as linear-log model, log-linear model and log-log model. As mentioned in \hyperref[sec:data]{Data} Section, we model \textit{Age} using quadratic polynomials and \textit{Wage} using logarithms. Hence, we use Equation (\ref{eq:mlr_polynomial}) of degree $2$ and Equation (\ref{eq:mlr_log_linear}) in the regression model. The \acs{OLS} estimators in the above models are obtained using Stata. We will show if it is reasonable to assume the \acs{OLS} assumptions in Appendix \ref{appendix: assumptions}. For details regarding the notations and mathematical representations, see Appendix \ref{appendix:notations}.